//! LLaMA Model Implementation
//!
//! Implements the LLaMA architecture (LLaMA 2/3, Code LLaMA, etc.) using
//! the shared building blocks from burn-models-core.
//!
//! # Architecture
//!
//! LLaMA uses a decoder-only transformer with:
//! - Pre-norm architecture (RMSNorm before attention and FFN)
//! - Rotary Position Embeddings (RoPE)
//! - SwiGLU activation in FFN
//! - Grouped-Query Attention (GQA) in LLaMA 2 70B and LLaMA 3
//!
//! # Supported Variants
//!
//! - LLaMA 2 7B, 13B, 70B
//! - LLaMA 3 8B, 70B
//! - Code LLaMA

use burn::prelude::*;
use burn::nn::{Embedding, EmbeddingConfig};

use burn_models_core::kv_cache::ModelKvCache;
use burn_models_core::rmsnorm::RmsNorm;
use burn_models_core::rope::RotaryEmbedding;
use burn_models_core::transformer::{TransformerBlock, TransformerBlockConfig, causal_mask};

/// LLaMA Model Configuration
#[derive(Debug, Clone)]
pub struct LlamaConfig {
    /// Vocabulary size
    pub vocab_size: usize,
    /// Hidden dimension
    pub hidden_size: usize,
    /// Intermediate (FFN) dimension
    pub intermediate_size: usize,
    /// Number of transformer layers
    pub num_layers: usize,
    /// Number of attention heads
    pub num_heads: usize,
    /// Number of KV heads (for GQA; equal to num_heads for MHA)
    pub num_kv_heads: usize,
    /// Maximum sequence length
    pub max_seq_len: usize,
    /// RMSNorm epsilon
    pub norm_eps: f64,
    /// RoPE base frequency
    pub rope_base: f32,
}

impl LlamaConfig {
    /// LLaMA 2 7B configuration
    pub fn llama2_7b() -> Self {
        Self {
            vocab_size: 32000,
            hidden_size: 4096,
            intermediate_size: 11008,
            num_layers: 32,
            num_heads: 32,
            num_kv_heads: 32, // MHA
            max_seq_len: 4096,
            norm_eps: 1e-5,
            rope_base: 10000.0,
        }
    }

    /// LLaMA 2 13B configuration
    pub fn llama2_13b() -> Self {
        Self {
            vocab_size: 32000,
            hidden_size: 5120,
            intermediate_size: 13824,
            num_layers: 40,
            num_heads: 40,
            num_kv_heads: 40, // MHA
            max_seq_len: 4096,
            norm_eps: 1e-5,
            rope_base: 10000.0,
        }
    }

    /// LLaMA 2 70B configuration (with GQA)
    pub fn llama2_70b() -> Self {
        Self {
            vocab_size: 32000,
            hidden_size: 8192,
            intermediate_size: 28672,
            num_layers: 80,
            num_heads: 64,
            num_kv_heads: 8, // GQA: 8 KV heads
            max_seq_len: 4096,
            norm_eps: 1e-5,
            rope_base: 10000.0,
        }
    }

    /// LLaMA 3 8B configuration
    pub fn llama3_8b() -> Self {
        Self {
            vocab_size: 128256,
            hidden_size: 4096,
            intermediate_size: 14336,
            num_layers: 32,
            num_heads: 32,
            num_kv_heads: 8, // GQA
            max_seq_len: 8192,
            norm_eps: 1e-5,
            rope_base: 500000.0, // Higher base for longer context
        }
    }

    /// LLaMA 3 70B configuration
    pub fn llama3_70b() -> Self {
        Self {
            vocab_size: 128256,
            hidden_size: 8192,
            intermediate_size: 28672,
            num_layers: 80,
            num_heads: 64,
            num_kv_heads: 8, // GQA
            max_seq_len: 8192,
            norm_eps: 1e-5,
            rope_base: 500000.0,
        }
    }

    /// Creates a tiny model for testing
    pub fn tiny() -> Self {
        Self {
            vocab_size: 1000,
            hidden_size: 128,
            intermediate_size: 256,
            num_layers: 2,
            num_heads: 4,
            num_kv_heads: 4,
            max_seq_len: 512,
            norm_eps: 1e-5,
            rope_base: 10000.0,
        }
    }

    /// Initialize the model and runtime
    pub fn init<B: Backend>(&self, device: &B::Device) -> (Llama<B>, LlamaRuntime<B>) {
        let head_dim = self.hidden_size / self.num_heads;

        let layers: Vec<TransformerBlock<B>> = (0..self.num_layers)
            .map(|_| {
                TransformerBlockConfig::with_gqa(
                    self.hidden_size,
                    self.intermediate_size,
                    self.num_heads,
                    self.num_kv_heads,
                )
                .init(device)
            })
            .collect();

        let model = Llama {
            embed_tokens: EmbeddingConfig::new(self.vocab_size, self.hidden_size).init(device),
            layers,
            norm: RmsNorm::with_eps(self.hidden_size, self.norm_eps, device),
            lm_head: burn::nn::LinearConfig::new(self.hidden_size, self.vocab_size)
                .with_bias(false)
                .init(device),
        };

        let runtime = LlamaRuntime {
            rope: RotaryEmbedding::with_base(head_dim, self.max_seq_len, self.rope_base, device),
            config: self.clone(),
        };

        (model, runtime)
    }
}

/// LLaMA Model
#[derive(Module, Debug)]
pub struct Llama<B: Backend> {
    embed_tokens: Embedding<B>,
    layers: Vec<TransformerBlock<B>>,
    norm: RmsNorm<B>,
    lm_head: burn::nn::Linear<B>,
}

/// Runtime state for LLaMA (not part of the module)
pub struct LlamaRuntime<B: Backend> {
    /// Rotary position embeddings
    pub rope: RotaryEmbedding<B>,
    /// Model configuration
    pub config: LlamaConfig,
}

/// Output from the LLaMA model
pub struct LlamaOutput<B: Backend> {
    /// Logits over vocabulary: [batch, seq_len, vocab_size]
    pub logits: Tensor<B, 3>,
    /// Hidden states from final layer: [batch, seq_len, hidden_size]
    pub hidden_states: Tensor<B, 3>,
}

impl<B: Backend> Llama<B> {
    /// Forward pass through the model
    ///
    /// # Arguments
    ///
    /// * `input_ids` - Input token IDs [batch, seq_len]
    /// * `runtime` - Model runtime containing RoPE and config
    /// * `cache` - Optional KV cache for incremental generation
    ///
    /// # Returns
    ///
    /// Model output containing logits and hidden states
    pub fn forward(
        &self,
        input_ids: Tensor<B, 2, Int>,
        runtime: &LlamaRuntime<B>,
        mut cache: Option<&mut ModelKvCache<B>>,
    ) -> LlamaOutput<B> {
        let [_batch, seq_len] = input_ids.dims();
        let device = input_ids.device();

        // Get position offset from cache
        let start_pos = cache.as_ref().map(|c| c.seq_len()).unwrap_or(0);

        // Token embeddings
        let mut hidden_states = self.embed_tokens.forward(input_ids);

        // Causal mask (only needed for prefill, not single token generation)
        let mask = if seq_len > 1 {
            Some(causal_mask::<B>(seq_len, &device))
        } else {
            None
        };

        // Process through transformer layers
        for (layer_idx, layer) in self.layers.iter().enumerate() {
            // TODO: integrate KV cache into TransformerBlock
            // For now, just pass through without caching
            let _ = cache.as_mut().map(|c| c.layer(layer_idx));

            hidden_states = layer.forward(hidden_states, Some(&runtime.rope), start_pos, mask.clone());
        }

        // Final norm
        hidden_states = self.norm.forward(hidden_states);

        // LM head projection
        let logits = self.lm_head.forward(hidden_states.clone());

        LlamaOutput {
            logits,
            hidden_states,
        }
    }

    /// Generate text autoregressively
    ///
    /// # Arguments
    ///
    /// * `input_ids` - Initial prompt token IDs [batch, prompt_len]
    /// * `runtime` - Model runtime containing RoPE and config
    /// * `max_new_tokens` - Maximum tokens to generate
    /// * `temperature` - Sampling temperature (1.0 = no scaling)
    ///
    /// # Returns
    ///
    /// Generated token IDs including the prompt
    pub fn generate(
        &self,
        input_ids: Tensor<B, 2, Int>,
        runtime: &LlamaRuntime<B>,
        max_new_tokens: usize,
        temperature: f32,
    ) -> Tensor<B, 2, Int> {
        let [batch, _prompt_len] = input_ids.dims();
        let mut all_tokens = input_ids;

        for _ in 0..max_new_tokens {
            // Forward pass
            let output = self.forward(all_tokens.clone(), runtime, None);

            // Get logits for last position: [batch, 1, vocab_size]
            let seq_len = all_tokens.dims()[1];
            let last_logits = output.logits.slice([0..batch, (seq_len - 1)..seq_len, 0..runtime.config.vocab_size]);
            // Reshape to [batch, vocab_size]
            let last_logits = last_logits.reshape([batch, runtime.config.vocab_size]);

            // Apply temperature
            let scaled_logits = if (temperature - 1.0).abs() > 1e-6 {
                last_logits / temperature
            } else {
                last_logits
            };

            // Greedy sampling: get index of max value
            // argmax(1) returns shape [batch]
            let next_token_indices = scaled_logits.argmax(1);
            // Reshape to [batch, 1] for concatenation
            let next_token = next_token_indices.reshape([batch, 1]);

            // Append to sequence
            all_tokens = Tensor::cat(vec![all_tokens, next_token], 1);
        }

        all_tokens
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use burn_ndarray::NdArray;

    type TestBackend = NdArray<f32>;

    #[test]
    fn test_llama_tiny_forward() {
        let device = Default::default();
        let config = LlamaConfig::tiny();
        let (model, runtime) = config.init::<TestBackend>(&device);

        let input_ids = Tensor::<TestBackend, 2, Int>::from_ints([[1, 2, 3, 4]], &device);
        let output = model.forward(input_ids, &runtime, None);

        assert_eq!(output.logits.dims(), [1, 4, 1000]);
        assert_eq!(output.hidden_states.dims(), [1, 4, 128]);
    }

    #[test]
    fn test_llama_generate() {
        let device = Default::default();
        let config = LlamaConfig::tiny();
        let (model, runtime) = config.init::<TestBackend>(&device);

        let prompt = Tensor::<TestBackend, 2, Int>::from_ints([[1, 2]], &device);
        let generated = model.generate(prompt, &runtime, 3, 1.0);

        assert_eq!(generated.dims(), [1, 5]); // 2 prompt + 3 generated
    }

    #[test]
    fn test_llama_configs() {
        // Just verify configs don't panic
        let _ = LlamaConfig::llama2_7b();
        let _ = LlamaConfig::llama2_13b();
        let _ = LlamaConfig::llama2_70b();
        let _ = LlamaConfig::llama3_8b();
        let _ = LlamaConfig::llama3_70b();
    }

    #[test]
    fn test_llama_batch() {
        let device = Default::default();
        let config = LlamaConfig::tiny();
        let (model, runtime) = config.init::<TestBackend>(&device);

        let input_ids = Tensor::<TestBackend, 2, Int>::from_ints(
            [[1, 2, 3], [4, 5, 6]],
            &device
        );
        let output = model.forward(input_ids, &runtime, None);

        assert_eq!(output.logits.dims(), [2, 3, 1000]);
    }
}
