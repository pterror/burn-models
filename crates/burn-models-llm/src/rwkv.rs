//! RWKV-7 "Goose" Model Implementation
//!
//! RWKV is an RNN architecture that achieves transformer-level performance with:
//! - Linear time complexity O(n) vs transformer's O(nÂ²)
//! - Constant memory (no KV cache needed)
//! - Infinite context length theoretically possible
//! - Parallelizable during training
//!
//! # Architecture
//!
//! RWKV-7 uses alternating Time-Mix and Channel-Mix layers:
//! - **Time-Mix**: Processes temporal dependencies via dynamic state evolution
//! - **Channel-Mix**: Feed-forward network with squared ReLU activation
//!
//! # Key Differences from Transformers
//!
//! - Uses LayerNorm (not RMSNorm)
//! - No attention mechanism - uses linear recurrence
//! - State is updated recurrently, not stored in KV cache

use burn::module::Param;
use burn::nn::{Embedding, EmbeddingConfig, LayerNorm, LayerNormConfig, Linear, LinearConfig};
use burn::prelude::*;

/// RWKV-7 Model Configuration
#[derive(Debug, Clone)]
pub struct RwkvConfig {
    /// Vocabulary size
    pub vocab_size: usize,
    /// Hidden dimension (n_embd)
    pub hidden_size: usize,
    /// Number of layers (n_layer)
    pub num_layers: usize,
    /// Number of attention heads
    pub num_heads: usize,
    /// Head dimension (hidden_size / num_heads)
    pub head_dim: usize,
    /// Intermediate (FFN) dimension multiplier (typically 4x)
    pub ffn_multiplier: usize,
    /// Layer norm epsilon
    pub layer_norm_eps: f64,
}

impl RwkvConfig {
    /// RWKV-7 0.1B configuration
    pub fn rwkv7_0_1b() -> Self {
        Self {
            vocab_size: 65536,
            hidden_size: 768,
            num_layers: 12,
            num_heads: 12,
            head_dim: 64,
            ffn_multiplier: 4,
            layer_norm_eps: 1e-5,
        }
    }

    /// RWKV-7 0.4B configuration
    pub fn rwkv7_0_4b() -> Self {
        Self {
            vocab_size: 65536,
            hidden_size: 1024,
            num_layers: 24,
            num_heads: 16,
            head_dim: 64,
            ffn_multiplier: 4,
            layer_norm_eps: 1e-5,
        }
    }

    /// RWKV-7 1.5B configuration
    pub fn rwkv7_1_5b() -> Self {
        Self {
            vocab_size: 65536,
            hidden_size: 2048,
            num_layers: 24,
            num_heads: 32,
            head_dim: 64,
            ffn_multiplier: 4,
            layer_norm_eps: 1e-5,
        }
    }

    /// RWKV-7 3B configuration
    pub fn rwkv7_3b() -> Self {
        Self {
            vocab_size: 65536,
            hidden_size: 2560,
            num_layers: 32,
            num_heads: 40,
            head_dim: 64,
            ffn_multiplier: 4,
            layer_norm_eps: 1e-5,
        }
    }

    /// RWKV-7 7B configuration
    pub fn rwkv7_7b() -> Self {
        Self {
            vocab_size: 65536,
            hidden_size: 4096,
            num_layers: 32,
            num_heads: 64,
            head_dim: 64,
            ffn_multiplier: 4,
            layer_norm_eps: 1e-5,
        }
    }

    /// Creates a tiny model for testing
    pub fn tiny() -> Self {
        Self {
            vocab_size: 1000,
            hidden_size: 64,
            num_layers: 2,
            num_heads: 4,
            head_dim: 16,
            ffn_multiplier: 4,
            layer_norm_eps: 1e-5,
        }
    }

    /// Initialize the model and runtime
    pub fn init<B: Backend>(&self, device: &B::Device) -> (Rwkv<B>, RwkvRuntime<B>) {
        let layers: Vec<RwkvBlock<B>> = (0..self.num_layers)
            .map(|layer_id| RwkvBlock::new(self, layer_id, device))
            .collect();

        let model = Rwkv {
            embed_tokens: EmbeddingConfig::new(self.vocab_size, self.hidden_size).init(device),
            layers,
            ln_out: LayerNormConfig::new(self.hidden_size)
                .with_epsilon(self.layer_norm_eps)
                .init(device),
            lm_head: LinearConfig::new(self.hidden_size, self.vocab_size)
                .with_bias(false)
                .init(device),
        };

        let runtime = RwkvRuntime {
            config: self.clone(),
            _marker: std::marker::PhantomData,
        };

        (model, runtime)
    }
}

/// RWKV-7 Time Mixing Layer
///
/// Implements the dynamic state evolution mechanism that replaces attention.
/// Uses linear recurrence with learned decay weights.
#[derive(Module, Debug)]
pub struct RwkvTimeMix<B: Backend> {
    /// Layer norm before time mix
    pub ln: LayerNorm<B>,
    /// Time shift mixing parameters (learnable)
    pub time_maa_x: Param<Tensor<B, 1>>,
    pub time_maa_r: Param<Tensor<B, 1>>,
    pub time_maa_w: Param<Tensor<B, 1>>,
    pub time_maa_k: Param<Tensor<B, 1>>,
    pub time_maa_v: Param<Tensor<B, 1>>,
    pub time_maa_a: Param<Tensor<B, 1>>,
    pub time_maa_g: Param<Tensor<B, 1>>,
    /// Decay parameters
    pub time_decay: Param<Tensor<B, 1>>,
    pub time_faaaa: Param<Tensor<B, 2>>,
    /// Projections
    pub receptance: Linear<B>,
    pub key: Linear<B>,
    pub value: Linear<B>,
    pub gate: Linear<B>,
    pub output: Linear<B>,
    /// Low-rank projections for dynamic mixing
    pub time_maa_w1: Param<Tensor<B, 2>>,
    pub time_maa_w2: Param<Tensor<B, 3>>,
    pub time_decay_w1: Param<Tensor<B, 2>>,
    pub time_decay_w2: Param<Tensor<B, 2>>,
    /// Group norm for output
    pub ln_x: burn::nn::GroupNorm<B>,
    #[module(skip)]
    pub num_heads: usize,
    #[module(skip)]
    pub head_dim: usize,
}

impl<B: Backend> RwkvTimeMix<B> {
    pub fn new(config: &RwkvConfig, _layer_id: usize, device: &B::Device) -> Self {
        let hidden = config.hidden_size;
        let num_heads = config.num_heads;
        let head_dim = config.head_dim;
        let inner_dim = num_heads * head_dim;

        // Low-rank dimension for dynamic mixing
        let lora_dim = 32.min(hidden / 4);

        Self {
            ln: LayerNormConfig::new(hidden)
                .with_epsilon(config.layer_norm_eps)
                .init(device),
            // Time mixing parameters initialized to reasonable defaults
            time_maa_x: Param::from_tensor(Tensor::zeros([hidden], device)),
            time_maa_r: Param::from_tensor(Tensor::zeros([hidden], device)),
            time_maa_w: Param::from_tensor(Tensor::zeros([hidden], device)),
            time_maa_k: Param::from_tensor(Tensor::zeros([hidden], device)),
            time_maa_v: Param::from_tensor(Tensor::zeros([hidden], device)),
            time_maa_a: Param::from_tensor(Tensor::zeros([hidden], device)),
            time_maa_g: Param::from_tensor(Tensor::zeros([hidden], device)),
            time_decay: Param::from_tensor(Tensor::zeros([inner_dim], device)),
            time_faaaa: Param::from_tensor(Tensor::zeros([num_heads, head_dim], device)),
            receptance: LinearConfig::new(hidden, inner_dim)
                .with_bias(false)
                .init(device),
            key: LinearConfig::new(hidden, inner_dim)
                .with_bias(false)
                .init(device),
            value: LinearConfig::new(hidden, inner_dim)
                .with_bias(false)
                .init(device),
            gate: LinearConfig::new(hidden, inner_dim)
                .with_bias(false)
                .init(device),
            output: LinearConfig::new(inner_dim, hidden)
                .with_bias(false)
                .init(device),
            // Low-rank projections
            time_maa_w1: Param::from_tensor(Tensor::zeros([hidden, lora_dim * 5], device)),
            time_maa_w2: Param::from_tensor(Tensor::zeros([5, lora_dim, hidden], device)),
            time_decay_w1: Param::from_tensor(Tensor::zeros([hidden, lora_dim], device)),
            time_decay_w2: Param::from_tensor(Tensor::zeros([lora_dim, inner_dim], device)),
            ln_x: burn::nn::GroupNormConfig::new(num_heads, inner_dim).init(device),
            num_heads,
            head_dim,
        }
    }

    /// Time shift operation - shifts input by 1 position
    fn time_shift(&self, x: Tensor<B, 3>, last_state: Option<Tensor<B, 2>>) -> Tensor<B, 3> {
        let [batch, seq_len, hidden] = x.dims();
        let device = x.device();

        if seq_len == 1 {
            // Single token - use last state if available
            match last_state {
                Some(state) => state.unsqueeze_dim(1),
                None => Tensor::zeros([batch, 1, hidden], &device),
            }
        } else {
            // Shift by 1: prepend zeros, remove last
            let zeros = Tensor::zeros([batch, 1, hidden], &device);
            let shifted = x.clone().slice([0..batch, 0..seq_len - 1, 0..hidden]);
            Tensor::cat(vec![zeros, shifted], 1)
        }
    }

    pub fn forward(&self, x: Tensor<B, 3>, mut state: Option<&mut RwkvState<B>>) -> Tensor<B, 3> {
        let [batch, seq_len, _] = x.dims();

        // Layer norm
        let x = self.ln.forward(x);

        // Get previous state for time shift
        let last_x = state.as_ref().map(|s| s.time_mix_x.clone());

        // Time shift
        let x_shifted = self.time_shift(x.clone(), last_x);

        // Update state with last token
        if let Some(ref mut s) = state {
            let last_token = x.clone().slice([
                0..batch,
                seq_len - 1..seq_len,
                0..self.num_heads * self.head_dim,
            ]);
            s.time_mix_x = last_token.squeeze_dim::<2>(1);
        }

        // Helper to unsqueeze [hidden] -> [1, 1, hidden] for broadcasting with [batch, seq, hidden]
        let unsqueeze_1d =
            |t: Tensor<B, 1>| -> Tensor<B, 3> { t.unsqueeze_dim::<2>(0).unsqueeze_dim::<3>(0) };

        // Compute time-mixed inputs using learnable mixing ratios
        let diff = x_shifted.clone() - x.clone();
        let x_maa = x.clone() + diff.clone() * unsqueeze_1d(self.time_maa_x.val());

        // Check if RWKV-7 dynamic mixing weights are present (non-zero)
        // RWKV-6 models won't have these, so we fall back to static mixing
        let w1_sum: f32 = self
            .time_maa_w1
            .val()
            .clone()
            .abs()
            .sum()
            .into_scalar()
            .elem();
        let use_dynamic_mixing = w1_sum > 1e-6;

        let (xr, xw, xk, xv, xg) = if use_dynamic_mixing {
            // RWKV-7 dynamic mixing via low-rank projection
            let hidden = self.num_heads * self.head_dim;
            let w1 = self.time_maa_w1.val();
            let [_, proj_dim] = w1.dims();
            let lora_dim = proj_dim / 5;

            // Project x_maa through W1: [batch*seq, hidden] @ [hidden, lora_dim*5] -> [batch*seq, lora_dim*5]
            let x_maa_flat = x_maa.reshape([batch * seq_len, hidden]);
            let maa_proj = x_maa_flat.matmul(w1);

            // Reshape to [batch, seq, 5, lora_dim], apply tanh
            let maa_proj = maa_proj.reshape([batch, seq_len, 5, lora_dim]);
            let maa_proj = maa_proj.tanh();

            // Project each of the 5 components back to hidden dim
            // time_maa_w2 is [5, lora_dim, hidden]
            let w2 = self.time_maa_w2.val();
            let dynamic_mix = |i: usize| -> Tensor<B, 3> {
                // Extract [batch, seq, lora_dim] for component i
                let component =
                    maa_proj
                        .clone()
                        .slice([0..batch, 0..seq_len, i..i + 1, 0..lora_dim]);
                let component = component.reshape([batch * seq_len, lora_dim]);
                // Extract [lora_dim, hidden] for W2[i]
                let w2_i = w2.clone().slice([i..i + 1, 0..lora_dim, 0..hidden]);
                let w2_i = w2_i.reshape([lora_dim, hidden]);
                // [batch*seq, lora_dim] @ [lora_dim, hidden] -> [batch*seq, hidden] -> [batch, seq, hidden]
                component.matmul(w2_i).reshape([batch, seq_len, hidden])
            };

            // Compute dynamic mixing ratios and apply to diff
            (
                x.clone() + diff.clone() * (unsqueeze_1d(self.time_maa_r.val()) + dynamic_mix(0)),
                x.clone() + diff.clone() * (unsqueeze_1d(self.time_maa_w.val()) + dynamic_mix(1)),
                x.clone() + diff.clone() * (unsqueeze_1d(self.time_maa_k.val()) + dynamic_mix(2)),
                x.clone() + diff.clone() * (unsqueeze_1d(self.time_maa_v.val()) + dynamic_mix(3)),
                x.clone() + diff * (unsqueeze_1d(self.time_maa_g.val()) + dynamic_mix(4)),
            )
        } else {
            // RWKV-6 static mixing (fallback when dynamic weights not present)
            (
                x.clone() + diff.clone() * unsqueeze_1d(self.time_maa_r.val()),
                x.clone() + diff.clone() * unsqueeze_1d(self.time_maa_w.val()),
                x.clone() + diff.clone() * unsqueeze_1d(self.time_maa_k.val()),
                x.clone() + diff.clone() * unsqueeze_1d(self.time_maa_v.val()),
                x.clone() + diff * unsqueeze_1d(self.time_maa_g.val()),
            )
        };
        let _ = xw; // Used for time decay computation in full implementation

        // Linear projections
        let r = self.receptance.forward(xr);
        let k = self.key.forward(xk);
        let v = self.value.forward(xv);
        let g = burn::tensor::activation::sigmoid(self.gate.forward(xg));

        // Compute decay weight - unsqueeze [inner_dim] -> [1, 1, inner_dim]
        let w = self
            .time_decay
            .val()
            .unsqueeze_dim::<2>(0)
            .unsqueeze_dim::<3>(0)
            .expand([batch, seq_len, self.num_heads * self.head_dim]);
        let w = (-w.exp()).exp(); // time decay

        // WKV computation (simplified - pure tensor ops, can be optimized with custom kernel)
        let wkv = self.wkv_forward(r.clone(), k, v, w, state);

        // Group norm and gating
        // GroupNorm expects [batch, channels, spatial...] so transpose [batch, seq, channels] -> [batch, channels, seq]
        let wkv = wkv.swap_dims(1, 2);
        let wkv = self.ln_x.forward(wkv);
        let wkv = wkv.swap_dims(1, 2); // Back to [batch, seq, channels]
        let out = wkv * g;

        // Output projection
        self.output.forward(out)
    }

    /// WKV (Weighted Key-Value) computation
    /// This is the core RWKV mechanism - a linear attention variant
    fn wkv_forward(
        &self,
        r: Tensor<B, 3>,
        k: Tensor<B, 3>,
        v: Tensor<B, 3>,
        w: Tensor<B, 3>,
        state: Option<&mut RwkvState<B>>,
    ) -> Tensor<B, 3> {
        let [batch, seq_len, inner_dim] = r.dims();
        let device = r.device();

        // Reshape to [batch, seq, heads, head_dim]
        let r = r.reshape([batch, seq_len, self.num_heads, self.head_dim]);
        let k = k.reshape([batch, seq_len, self.num_heads, self.head_dim]);
        let v = v.reshape([batch, seq_len, self.num_heads, self.head_dim]);
        let w = w.reshape([batch, seq_len, self.num_heads, self.head_dim]);

        // Initialize or get state
        let (mut wkv_state, wkv_scale) = match &state {
            Some(s) => (s.wkv_state.clone(), s.wkv_scale.clone()),
            None => (
                Tensor::zeros(
                    [batch, self.num_heads, self.head_dim, self.head_dim],
                    &device,
                ),
                Tensor::zeros([batch, self.num_heads, self.head_dim], &device),
            ),
        };

        // Process sequence (can be parallelized with chunking for training)
        let mut outputs = Vec::with_capacity(seq_len);

        for t in 0..seq_len {
            let r_t = r
                .clone()
                .slice([0..batch, t..t + 1, 0..self.num_heads, 0..self.head_dim])
                .squeeze_dim::<3>(1);
            let k_t = k
                .clone()
                .slice([0..batch, t..t + 1, 0..self.num_heads, 0..self.head_dim])
                .squeeze_dim::<3>(1);
            let v_t = v
                .clone()
                .slice([0..batch, t..t + 1, 0..self.num_heads, 0..self.head_dim])
                .squeeze_dim::<3>(1);
            let w_t = w
                .clone()
                .slice([0..batch, t..t + 1, 0..self.num_heads, 0..self.head_dim])
                .squeeze_dim::<3>(1);

            // Update state: S_t = w * S_{t-1} + k^T @ v (outer product)
            let kv = k_t
                .clone()
                .unsqueeze_dim::<4>(3)
                .matmul(v_t.clone().unsqueeze_dim::<4>(2));
            let w_expanded = w_t.clone().unsqueeze_dim::<4>(3);
            wkv_state = wkv_state * w_expanded + kv;

            // Compute output: r @ S_t
            let out_t = r_t
                .unsqueeze_dim::<4>(2)
                .matmul(wkv_state.clone())
                .squeeze_dim::<3>(2);
            outputs.push(out_t.unsqueeze_dim::<4>(1));
        }

        // Update state
        if let Some(s) = state {
            s.wkv_state = wkv_state;
            s.wkv_scale = wkv_scale;
        }

        // Concatenate outputs
        let output = Tensor::cat(outputs, 1);
        output.reshape([batch, seq_len, inner_dim])
    }
}

/// RWKV-7 Channel Mixing (FFN) Layer
///
/// Simple feed-forward network with squared ReLU activation.
#[derive(Module, Debug)]
pub struct RwkvChannelMix<B: Backend> {
    /// Layer norm before channel mix
    pub ln: LayerNorm<B>,
    /// Time shift mixing parameter
    pub time_maa_k: Param<Tensor<B, 1>>,
    /// Key projection (expansion)
    pub key: Linear<B>,
    /// Value projection (contraction)
    pub value: Linear<B>,
}

impl<B: Backend> RwkvChannelMix<B> {
    pub fn new(config: &RwkvConfig, device: &B::Device) -> Self {
        let hidden = config.hidden_size;
        let intermediate = hidden * config.ffn_multiplier;

        Self {
            ln: LayerNormConfig::new(hidden)
                .with_epsilon(config.layer_norm_eps)
                .init(device),
            time_maa_k: Param::from_tensor(Tensor::zeros([hidden], device)),
            key: LinearConfig::new(hidden, intermediate)
                .with_bias(false)
                .init(device),
            value: LinearConfig::new(intermediate, hidden)
                .with_bias(false)
                .init(device),
        }
    }

    /// Time shift operation
    fn time_shift(&self, x: Tensor<B, 3>, last_state: Option<Tensor<B, 2>>) -> Tensor<B, 3> {
        let [batch, seq_len, hidden] = x.dims();
        let device = x.device();

        if seq_len == 1 {
            match last_state {
                Some(state) => state.unsqueeze_dim(1),
                None => Tensor::zeros([batch, 1, hidden], &device),
            }
        } else {
            let zeros = Tensor::zeros([batch, 1, hidden], &device);
            let shifted = x.clone().slice([0..batch, 0..seq_len - 1, 0..hidden]);
            Tensor::cat(vec![zeros, shifted], 1)
        }
    }

    pub fn forward(&self, x: Tensor<B, 3>, state: Option<&mut RwkvState<B>>) -> Tensor<B, 3> {
        let [batch, seq_len, hidden] = x.dims();

        // Layer norm
        let x = self.ln.forward(x);

        // Get previous state for time shift
        let last_x = state.as_ref().map(|s| s.channel_mix_x.clone());

        // Time shift
        let x_shifted = self.time_shift(x.clone(), last_x);

        // Update state with last token
        if let Some(s) = state {
            let last_token = x.clone().slice([0..batch, seq_len - 1..seq_len, 0..hidden]);
            s.channel_mix_x = last_token.squeeze_dim::<2>(1);
        }

        // Mixed input - unsqueeze [hidden] -> [1, 1, hidden] for broadcasting
        let time_maa = self
            .time_maa_k
            .val()
            .unsqueeze_dim::<2>(0)
            .unsqueeze_dim::<3>(0);
        let xk = x.clone() + (x_shifted - x) * time_maa;

        // FFN: key -> squared_relu -> value
        let k = self.key.forward(xk);
        let k = burn::tensor::activation::relu(k);
        let k = k.clone() * k; // Squared ReLU

        self.value.forward(k)
    }
}

/// RWKV Block (Time Mix + Channel Mix)
#[derive(Module, Debug)]
pub struct RwkvBlock<B: Backend> {
    pub time_mix: RwkvTimeMix<B>,
    pub channel_mix: RwkvChannelMix<B>,
}

impl<B: Backend> RwkvBlock<B> {
    pub fn new(config: &RwkvConfig, layer_id: usize, device: &B::Device) -> Self {
        Self {
            time_mix: RwkvTimeMix::new(config, layer_id, device),
            channel_mix: RwkvChannelMix::new(config, device),
        }
    }

    pub fn forward(&self, x: Tensor<B, 3>, state: Option<&mut RwkvState<B>>) -> Tensor<B, 3> {
        // Split the mutable borrow for time_mix and channel_mix
        let (time_state, channel_state) = match state {
            Some(s) => {
                // We need to pass state to both, but can't split &mut
                // So we pass to time_mix first, then channel_mix
                (Some(s), None::<&mut RwkvState<B>>)
            }
            None => (None, None),
        };

        // Time mix with residual
        let x = x.clone() + self.time_mix.forward(x.clone(), time_state);
        // Channel mix with residual (state already updated by time_mix)
        x.clone() + self.channel_mix.forward(x, channel_state)
    }
}

/// RWKV State for recurrent inference
///
/// Stores the hidden state between forward passes.
#[derive(Debug, Clone)]
pub struct RwkvState<B: Backend> {
    /// Last token for time mix time shift
    pub time_mix_x: Tensor<B, 2>,
    /// Last token for channel mix time shift
    pub channel_mix_x: Tensor<B, 2>,
    /// WKV state matrix [batch, heads, head_dim, head_dim]
    pub wkv_state: Tensor<B, 4>,
    /// WKV scale for numerical stability
    pub wkv_scale: Tensor<B, 3>,
}

impl<B: Backend> RwkvState<B> {
    pub fn new(config: &RwkvConfig, batch: usize, device: &B::Device) -> Self {
        let inner_dim = config.num_heads * config.head_dim;
        Self {
            time_mix_x: Tensor::zeros([batch, inner_dim], device),
            channel_mix_x: Tensor::zeros([batch, config.hidden_size], device),
            wkv_state: Tensor::zeros(
                [batch, config.num_heads, config.head_dim, config.head_dim],
                device,
            ),
            wkv_scale: Tensor::zeros([batch, config.num_heads, config.head_dim], device),
        }
    }
}

/// RWKV-7 Model
#[derive(Module, Debug)]
pub struct Rwkv<B: Backend> {
    /// Token embeddings
    pub embed_tokens: Embedding<B>,
    /// RWKV blocks
    pub layers: Vec<RwkvBlock<B>>,
    /// Final layer norm
    pub ln_out: LayerNorm<B>,
    /// Language model head
    pub lm_head: Linear<B>,
}

/// Runtime configuration for RWKV (not part of Module)
pub struct RwkvRuntime<B: Backend> {
    pub config: RwkvConfig,
    pub _marker: std::marker::PhantomData<B>,
}

/// Output from the RWKV model
pub struct RwkvOutput<B: Backend> {
    /// Logits over vocabulary: [batch, seq_len, vocab_size]
    pub logits: Tensor<B, 3>,
    /// Hidden states from final layer
    pub hidden_states: Tensor<B, 3>,
}

impl<B: Backend> Rwkv<B> {
    /// Forward pass through the model
    pub fn forward(
        &self,
        input_ids: Tensor<B, 2, Int>,
        mut states: Option<&mut Vec<RwkvState<B>>>,
    ) -> RwkvOutput<B> {
        // Embed tokens
        let mut hidden_states = self.embed_tokens.forward(input_ids);

        // Process through layers
        for (layer_idx, layer) in self.layers.iter().enumerate() {
            let state = states.as_mut().map(|s| &mut s[layer_idx]);
            hidden_states = layer.forward(hidden_states, state);
        }

        // Final norm
        hidden_states = self.ln_out.forward(hidden_states);

        // Project to vocabulary
        let logits = self.lm_head.forward(hidden_states.clone());

        RwkvOutput {
            logits,
            hidden_states,
        }
    }

    /// Initialize fresh states for recurrent inference
    pub fn init_states(
        &self,
        runtime: &RwkvRuntime<B>,
        batch: usize,
        device: &B::Device,
    ) -> Vec<RwkvState<B>> {
        (0..runtime.config.num_layers)
            .map(|_| RwkvState::new(&runtime.config, batch, device))
            .collect()
    }

    /// Generate text autoregressively
    pub fn generate(
        &self,
        input_ids: Tensor<B, 2, Int>,
        runtime: &RwkvRuntime<B>,
        max_new_tokens: usize,
        temperature: f32,
    ) -> Tensor<B, 2, Int> {
        let [batch, _] = input_ids.dims();
        let device = input_ids.device();

        // Initialize states
        let mut states = self.init_states(runtime, batch, &device);

        // Process prompt
        let output = self.forward(input_ids.clone(), Some(&mut states));

        // Get last token prediction
        let [_, seq_len, vocab_size] = output.logits.dims();
        let mut last_logits = output
            .logits
            .slice([0..batch, seq_len - 1..seq_len, 0..vocab_size])
            .squeeze_dim::<2>(1);

        let mut all_tokens = input_ids;

        // Generate new tokens
        for _ in 0..max_new_tokens {
            let scaled_logits = if (temperature - 1.0).abs() > 1e-6 {
                last_logits.clone() / temperature
            } else {
                last_logits.clone()
            };

            let next_token = scaled_logits.argmax(1).reshape([batch, 1]);
            all_tokens = Tensor::cat(vec![all_tokens, next_token.clone()], 1);

            // Forward single token with state
            let output = self.forward(next_token, Some(&mut states));
            last_logits = output.logits.squeeze_dim::<2>(1);
        }

        all_tokens
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use burn_ndarray::NdArray;

    type TestBackend = NdArray<f32>;

    #[test]
    fn test_rwkv_config() {
        let _ = RwkvConfig::rwkv7_0_1b();
        let _ = RwkvConfig::rwkv7_1_5b();
        let _ = RwkvConfig::rwkv7_3b();
        let _ = RwkvConfig::rwkv7_7b();
    }

    #[test]
    fn test_rwkv_tiny_forward() {
        let device = Default::default();
        let config = RwkvConfig::tiny();
        let (model, _runtime) = config.init::<TestBackend>(&device);

        let input_ids = Tensor::<TestBackend, 2, Int>::from_ints([[1, 2, 3, 4]], &device);
        let output = model.forward(input_ids, None);

        assert_eq!(output.logits.dims(), [1, 4, 1000]);
        assert_eq!(output.hidden_states.dims(), [1, 4, 64]);
    }

    #[test]
    fn test_rwkv_with_state() {
        let device = Default::default();
        let config = RwkvConfig::tiny();
        let (model, runtime) = config.init::<TestBackend>(&device);

        let mut states = model.init_states(&runtime, 1, &device);

        // First forward pass
        let input1 = Tensor::<TestBackend, 2, Int>::from_ints([[1, 2]], &device);
        let _ = model.forward(input1, Some(&mut states));

        // Second forward pass (incremental)
        let input2 = Tensor::<TestBackend, 2, Int>::from_ints([[3]], &device);
        let output = model.forward(input2, Some(&mut states));

        assert_eq!(output.logits.dims(), [1, 1, 1000]);
    }

    #[test]
    fn test_rwkv_generate() {
        let device = Default::default();
        let config = RwkvConfig::tiny();
        let (model, runtime) = config.init::<TestBackend>(&device);

        let prompt = Tensor::<TestBackend, 2, Int>::from_ints([[1, 2]], &device);
        let generated = model.generate(prompt, &runtime, 3, 1.0);

        assert_eq!(generated.dims(), [1, 5]);
    }

    #[test]
    fn test_rwkv_channel_mix() {
        let device = Default::default();
        let config = RwkvConfig::tiny();
        let channel_mix = RwkvChannelMix::new(&config, &device);

        let x = Tensor::<TestBackend, 3>::zeros([1, 4, 64], &device);
        let output = channel_mix.forward(x, None);

        assert_eq!(output.dims(), [1, 4, 64]);
    }

    #[test]
    fn test_rwkv_time_mix() {
        let device = Default::default();
        let config = RwkvConfig::tiny();
        let time_mix = RwkvTimeMix::new(&config, 0, &device);

        let x = Tensor::<TestBackend, 3>::zeros([1, 4, 64], &device);
        let output = time_mix.forward(x, None);

        assert_eq!(output.dims(), [1, 4, 64]);
    }
}
